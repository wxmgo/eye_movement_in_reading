{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考https://blog.csdn.net/u010041824/article/details/78855435  by BabY虎子\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PARTICIPANT_ID='Sub01'\n",
    "\n",
    "df_csv=pd.read_csv('data/Provo_Corpus-Eyetracking_Data(sub1-10).csv',\n",
    "                   usecols=['Participant_ID','Text_ID','Word_Cleaned','IA_SKIP','Word_Length'])\n",
    "df_csv=df_csv[(df_csv['Participant_ID']==PARTICIPANT_ID)]\n",
    "df_csv=df_csv.drop(['Participant_ID'],axis=1) #删除列，轴为1\n",
    "df_csv['Word_Length']=df_csv['Word_Length'].fillna('0')\n",
    "df_csv['Word_Length']=df_csv['Word_Length'].astype('int32')\n",
    "df_csv=df_csv.fillna(' ')\n",
    "\n",
    "list_data=[]\n",
    "for j in range(1,56): #句子1-55\n",
    "    df=df_csv[(df_csv['Text_ID']==j)][['Word_Cleaned','IA_SKIP','Word_Length']] #只保留'Word_Cleaned','IA_SKIP'两列\n",
    "    mylist = np.array(df).tolist()\n",
    "    list_data.append(mylist)\n",
    "\n",
    "texts=[[j[0] for j in i]for i in list_data]\n",
    "labels=[[j[1] for j in i]for i in list_data]\n",
    "wl=[[j[2] for j in i]for i in list_data]\n",
    "#print(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0]\n",
      "  [   0]\n",
      "  [   0]\n",
      "  ...\n",
      "  [   3]\n",
      "  [ 340]\n",
      "  [   6]]\n",
      "\n",
      " [[   0]\n",
      "  [ 160]\n",
      "  [ 161]\n",
      "  ...\n",
      "  [  15]\n",
      "  [ 363]\n",
      "  [   6]]\n",
      "\n",
      " [[   0]\n",
      "  [   0]\n",
      "  [ 364]\n",
      "  ...\n",
      "  [ 377]\n",
      "  [ 378]\n",
      "  [   6]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[   0]\n",
      "  [   0]\n",
      "  [   0]\n",
      "  ...\n",
      "  [  32]\n",
      "  [  54]\n",
      "  [   6]]\n",
      "\n",
      " [[   0]\n",
      "  [   0]\n",
      "  [   0]\n",
      "  ...\n",
      "  [1172]\n",
      "  [ 120]\n",
      "  [   6]]\n",
      "\n",
      " [[ 184]\n",
      "  [1173]\n",
      "  [1174]\n",
      "  ...\n",
      "  [1192]\n",
      "  [   6]\n",
      "  [   6]]]\n",
      "(55, 60, 1)\n",
      "[[[-1]\n",
      "  [-1]\n",
      "  [-1]\n",
      "  ...\n",
      "  [ 1]\n",
      "  [ 0]\n",
      "  [ 0]]\n",
      "\n",
      " [[-1]\n",
      "  [ 0]\n",
      "  [ 0]\n",
      "  ...\n",
      "  [ 1]\n",
      "  [ 0]\n",
      "  [ 0]]\n",
      "\n",
      " [[-1]\n",
      "  [-1]\n",
      "  [ 0]\n",
      "  ...\n",
      "  [ 0]\n",
      "  [ 0]\n",
      "  [ 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1]\n",
      "  [-1]\n",
      "  [-1]\n",
      "  ...\n",
      "  [ 1]\n",
      "  [ 0]\n",
      "  [ 0]]\n",
      "\n",
      " [[-1]\n",
      "  [-1]\n",
      "  [-1]\n",
      "  ...\n",
      "  [ 0]\n",
      "  [ 1]\n",
      "  [ 0]]\n",
      "\n",
      " [[ 1]\n",
      "  [ 1]\n",
      "  [ 1]\n",
      "  ...\n",
      "  [ 0]\n",
      "  [ 1]\n",
      "  [ 0]]]\n",
      "(55, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS = 2000 #字典大小\n",
    "MAX_SEQUENCE_LENGTH = max(len(s) for s in texts)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "MAX_NB_WORDS = len(word_index)+1 #重新修正字典大小\n",
    "#print(word_index)\n",
    "x = pad_sequences(sequences, MAX_SEQUENCE_LENGTH)  # left padding\n",
    "x = np.expand_dims(x, 2) \n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = pad_sequences(labels, MAX_SEQUENCE_LENGTH, value=-1)  # left padding\n",
    "y = np.expand_dims(y, 2) \n",
    "#这是因为crf层期望标签具有不同的形状。\n",
    "#通常，我们的标签的shape为(num_samples,max_length)，但crf层需要的shape为(num_samples,max_length,1)。\n",
    "#解决方法：将标签扩一维\n",
    "#https://blog.csdn.net/qq_31456593/article/details/89578018\n",
    "\n",
    "#用Dense做激活层的时候，仍然需要这个expand_dims。\n",
    "wl = pad_sequences(wl, MAX_SEQUENCE_LENGTH, value=-1)  # left padding\n",
    "wl = np.expand_dims(wl, 2) \n",
    "\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "train_x=x[0:45]\n",
    "train_y=y[0:45]\n",
    "train_wl=wl[0:45]\n",
    "test_x=x[45:]\n",
    "test_y=y[45:]\n",
    "test_wl=wl[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 1, 60)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 60)        0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 60)        3660        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 60, 1)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Concatenate)     (None, 60, 2)        0           input_1[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 60, 32)       4480        attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "world_length_input (InputLayer) (None, 60, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 33)       0           lstm_1[0][0]                     \n",
      "                                                                 world_length_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 60, 1)        34          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,174\n",
      "Trainable params: 8,174\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers import Embedding, Bidirectional, LSTM,Dense, Conv1D, MaxPooling1D,TimeDistributed,Flatten\n",
    "\n",
    "INPUT_DIM = 1\n",
    "TIME_STEPS = MAX_SEQUENCE_LENGTH\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = concatenate([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "def model_attention_applied_after_lstm(inputs):\n",
    "#    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    return attention_mul\n",
    "#    attention_mul = Flatten()(attention_mul)\n",
    "#    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "#    model = Model(input=[inputs], output=output)\n",
    "#    return model\n",
    "\n",
    "def model_attention_applied_before_lstm(inputs):\n",
    "#    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=True)(attention_mul)#原例子中是return_sequences=False\n",
    "    return attention_mul\n",
    "#    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "#    model = Model(input=[inputs], output=output)\n",
    "#    return model\n",
    "\n",
    "#model = model_attention_applied_after_lstm()\n",
    "#model = model_attention_applied_before_lstm()\n",
    "#inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "#lstm_units = 32\n",
    "#lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "attention_mul = model_attention_applied_before_lstm(inputs)\n",
    "#attention_mul = model_attention_applied_after_lstm(inputs)\n",
    "\n",
    "Word_Length_input = Input(shape=(TIME_STEPS, INPUT_DIM,), name='world_length_input')\n",
    "x =concatenate([attention_mul, Word_Length_input])\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(input=[inputs,Word_Length_input], output=output)\n",
    "    \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 45 samples, validate on 10 samples\n",
      "Epoch 1/15\n",
      "45/45 [==============================] - 2s 40ms/step - loss: 0.6236 - acc: 0.5548 - val_loss: 0.6668 - val_acc: 0.5117\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5780 - acc: 0.5530 - val_loss: 0.6286 - val_acc: 0.5467\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5315 - acc: 0.5811 - val_loss: 0.5968 - val_acc: 0.5817\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.4350 - acc: 0.5856 - val_loss: 0.5396 - val_acc: 0.5933\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.2466 - acc: 0.5907 - val_loss: 0.4491 - val_acc: 0.5833\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -3.8895e-04 - acc: 0.5789 - val_loss: 0.3460 - val_acc: 0.5783\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 0s 11ms/step - loss: -0.2224 - acc: 0.5767 - val_loss: 0.2315 - val_acc: 0.5800\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -0.4554 - acc: 0.5789 - val_loss: 0.1239 - val_acc: 0.5700\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -0.6408 - acc: 0.5744 - val_loss: 0.0404 - val_acc: 0.5667\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -0.7949 - acc: 0.5789 - val_loss: -0.0388 - val_acc: 0.5717\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -0.9014 - acc: 0.5815 - val_loss: -0.1046 - val_acc: 0.5733\n",
      "Epoch 12/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -1.0159 - acc: 0.5833 - val_loss: -0.1644 - val_acc: 0.5750\n",
      "Epoch 13/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -1.1195 - acc: 0.5833 - val_loss: -0.2246 - val_acc: 0.5767\n",
      "Epoch 14/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -1.2161 - acc: 0.5848 - val_loss: -0.2796 - val_acc: 0.5767\n",
      "Epoch 15/15\n",
      "45/45 [==============================] - 0s 10ms/step - loss: -1.3060 - acc: 0.5819 - val_loss: -0.3342 - val_acc: 0.5733\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "# train model\n",
    "train_history=model.fit([train_x,train_wl], train_y,batch_size=4,epochs=EPOCHS,  validation_data=([test_x,test_wl], test_y))\n",
    "model.save('model/crf.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_train_history(train_history,train,validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_history(train_history,'acc','val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
